{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebcf31be-2c72-4f3b-8e8e-4d4df20eec01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from processing_image import Preprocess\n",
    "from modeling_frcnn import GeneralizedRCNN\n",
    "from utils import Config\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f639b7-5fa5-4e11-9dde-e594c519a460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Image\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a96d43-0d18-4750-893b-7b51ded57ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561d4619-ad18-467f-8412-a020e7c69011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22185457-c800-4aaa-9283-77c6414e1bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /nas/home/bchidamb/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load models and model components\n",
    "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "frcnn_cfg.model.DEVICE = device\n",
    "\n",
    "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg).to(device)\n",
    "\n",
    "image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "# bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "# visualbert_vqa = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5973abc-974e-4b1a-959f-64fc7c191ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar10 (/nas/home/bchidamb/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bc6d4fd90c4365bf06630a2be1a7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar10_data = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd5b6644-b809-403c-9ce8-9077529028fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = cifar10_data[\"train\"]\n",
    "test_data = cifar10_data[\"test\"]\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "211a7367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7F456D632380>, 'label': 0}\n",
      "On idx 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('obj_ids',\n",
       "              tensor([[ 106,   72,  395,   72,  106,  364,  364,  364,  395,  956,  106,  106,\n",
       "                        395,  106,  395,  106,  956,   71,   72,  395,  364,  956,  395,   72,\n",
       "                        106,  395,  395,  395,   72,  106,  956,  177,  106,  242,  395, 1547]])),\n",
       "             ('obj_probs',\n",
       "              tensor([[0.4995, 0.4922, 0.4374, 0.4207, 0.4112, 0.4067, 0.3885, 0.3468, 0.3144,\n",
       "                       0.2768, 0.2723, 0.2403, 0.2272, 0.2226, 0.2132, 0.2031, 0.1965, 0.1931,\n",
       "                       0.1884, 0.1879, 0.1859, 0.1817, 0.1760, 0.1739, 0.1732, 0.1623, 0.1539,\n",
       "                       0.1515, 0.1485, 0.1362, 0.1222, 0.1221, 0.1163, 0.1112, 0.1088, 0.0892]])),\n",
       "             ('attr_ids',\n",
       "              tensor([[210,   7, 210,   7, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210,\n",
       "                       210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210, 210,\n",
       "                       210, 210, 210, 210, 210, 210, 210, 210]])),\n",
       "             ('attr_probs',\n",
       "              tensor([[0.5277, 0.2920, 0.9726, 0.3660, 0.7805, 0.7020, 0.7408, 0.7865, 0.9722,\n",
       "                       0.7582, 0.7106, 0.2529, 0.9615, 0.9481, 0.9489, 0.9323, 0.8268, 0.9806,\n",
       "                       0.6865, 0.9703, 0.7940, 0.8691, 0.9789, 0.4905, 0.9248, 0.9394, 0.9813,\n",
       "                       0.9717, 0.7315, 0.7653, 0.8295, 0.9309, 0.9341, 0.6940, 0.9489, 0.9611]])),\n",
       "             ('boxes',\n",
       "              tensor([[[1.1493e+00, 2.5676e+01, 2.5914e+01, 3.1288e+01],\n",
       "                       [1.2216e+00, 0.0000e+00, 2.8517e+01, 5.7680e+00],\n",
       "                       [7.0444e+00, 3.7361e-01, 2.7500e+01, 2.9987e+01],\n",
       "                       [1.3124e+00, 0.0000e+00, 2.9093e+01, 3.7756e+00],\n",
       "                       [5.7068e+00, 2.4980e+01, 2.9388e+01, 3.1332e+01],\n",
       "                       [3.1563e-01, 1.8553e+01, 1.3837e+01, 2.5729e+01],\n",
       "                       [1.5045e+00, 1.7545e+01, 1.1812e+01, 2.6201e+01],\n",
       "                       [2.8914e+00, 1.8243e+01, 1.3374e+01, 2.6614e+01],\n",
       "                       [4.8120e+00, 1.7809e-01, 2.4785e+01, 2.6166e+01],\n",
       "                       [9.0657e-01, 5.3653e-02, 3.1730e+01, 1.0608e+01],\n",
       "                       [0.0000e+00, 2.4096e+01, 2.8765e+01, 3.1871e+01],\n",
       "                       [0.0000e+00, 2.6972e+01, 2.8768e+01, 3.2000e+01],\n",
       "                       [9.7886e+00, 1.3693e-01, 3.0294e+01, 2.5658e+01],\n",
       "                       [5.6498e+00, 2.3957e+01, 3.0587e+01, 3.0057e+01],\n",
       "                       [3.8670e+00, 8.3610e-01, 3.2000e+01, 2.0956e+01],\n",
       "                       [5.8246e+00, 2.0860e+01, 3.2000e+01, 3.1757e+01],\n",
       "                       [1.8175e-01, 1.4140e-01, 2.7341e+01, 1.4230e+01],\n",
       "                       [6.6259e+00, 5.6762e+00, 3.2000e+01, 2.7053e+01],\n",
       "                       [0.0000e+00, 4.5638e-02, 2.2783e+01, 1.0885e+01],\n",
       "                       [4.5915e+00, 5.1790e+00, 2.5739e+01, 3.2000e+01],\n",
       "                       [1.6180e+00, 1.9978e+01, 1.7529e+01, 2.6865e+01],\n",
       "                       [6.5213e+00, 1.5826e-01, 3.0530e+01, 1.2901e+01],\n",
       "                       [0.0000e+00, 6.0216e+00, 2.8698e+01, 2.8388e+01],\n",
       "                       [0.0000e+00, 3.0858e-02, 1.7925e+01, 9.4976e+00],\n",
       "                       [0.0000e+00, 2.0573e+01, 2.8672e+01, 3.1771e+01],\n",
       "                       [0.0000e+00, 1.2088e+00, 2.7300e+01, 2.2150e+01],\n",
       "                       [0.0000e+00, 3.3553e+00, 2.1668e+01, 2.7290e+01],\n",
       "                       [0.0000e+00, 1.0249e+01, 2.6659e+01, 3.1140e+01],\n",
       "                       [5.8933e+00, 8.2264e-01, 3.0419e+01, 7.6164e+00],\n",
       "                       [0.0000e+00, 2.4085e+01, 2.3158e+01, 2.9781e+01],\n",
       "                       [9.7510e+00, 1.1226e-01, 3.2000e+01, 1.0933e+01],\n",
       "                       [0.0000e+00, 6.7942e-01, 1.9660e+01, 2.1227e+01],\n",
       "                       [2.4205e+00, 1.7419e+01, 3.1940e+01, 3.1662e+01],\n",
       "                       [4.8339e+00, 2.1429e+01, 1.2057e+01, 2.6339e+01],\n",
       "                       [1.4247e+01, 1.3446e+00, 3.2000e+01, 3.1539e+01],\n",
       "                       [4.9906e+00, 2.3340e+01, 3.1568e+01, 2.8556e+01]]])),\n",
       "             ('sizes', tensor([[800, 800]])),\n",
       "             ('preds_per_image', tensor([36])),\n",
       "             ('roi_features',\n",
       "              tensor([[[0.0000e+00, 2.1571e-01, 5.1866e-02,  ..., 0.0000e+00,\n",
       "                        2.5036e-02, 9.6567e-02],\n",
       "                       [8.3388e-03, 2.8794e+00, 5.4044e-02,  ..., 3.1142e-01,\n",
       "                        3.0753e+00, 0.0000e+00],\n",
       "                       [0.0000e+00, 1.0598e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "                        1.2320e-02, 7.8854e-01],\n",
       "                       ...,\n",
       "                       [1.3768e-02, 6.2635e-01, 3.1005e-01,  ..., 0.0000e+00,\n",
       "                        0.0000e+00, 0.0000e+00],\n",
       "                       [0.0000e+00, 1.2265e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "                        3.0831e-01, 4.8339e-01],\n",
       "                       [0.0000e+00, 3.2011e+00, 2.0053e-03,  ..., 8.2449e-02,\n",
       "                        2.2518e+00, 7.1040e-03]]])),\n",
       "             ('normalized_boxes',\n",
       "              tensor([[[3.5916e-02, 8.0237e-01, 8.0980e-01, 9.7774e-01],\n",
       "                       [3.8175e-02, 0.0000e+00, 8.9115e-01, 1.8025e-01],\n",
       "                       [2.2014e-01, 1.1675e-02, 8.5939e-01, 9.3709e-01],\n",
       "                       [4.1013e-02, 0.0000e+00, 9.0915e-01, 1.1799e-01],\n",
       "                       [1.7834e-01, 7.8061e-01, 9.1838e-01, 9.7913e-01],\n",
       "                       [9.8635e-03, 5.7977e-01, 4.3240e-01, 8.0403e-01],\n",
       "                       [4.7017e-02, 5.4829e-01, 3.6911e-01, 8.1878e-01],\n",
       "                       [9.0356e-02, 5.7009e-01, 4.1793e-01, 8.3168e-01],\n",
       "                       [1.5037e-01, 5.5653e-03, 7.7452e-01, 8.1770e-01],\n",
       "                       [2.8330e-02, 1.6767e-03, 9.9156e-01, 3.3149e-01],\n",
       "                       [0.0000e+00, 7.5301e-01, 8.9891e-01, 9.9597e-01],\n",
       "                       [0.0000e+00, 8.4288e-01, 8.9901e-01, 1.0000e+00],\n",
       "                       [3.0589e-01, 4.2791e-03, 9.4669e-01, 8.0182e-01],\n",
       "                       [1.7656e-01, 7.4866e-01, 9.5583e-01, 9.3927e-01],\n",
       "                       [1.2084e-01, 2.6128e-02, 1.0000e+00, 6.5488e-01],\n",
       "                       [1.8202e-01, 6.5187e-01, 1.0000e+00, 9.9242e-01],\n",
       "                       [5.6797e-03, 4.4188e-03, 8.5439e-01, 4.4469e-01],\n",
       "                       [2.0706e-01, 1.7738e-01, 1.0000e+00, 8.4542e-01],\n",
       "                       [0.0000e+00, 1.4262e-03, 7.1198e-01, 3.4016e-01],\n",
       "                       [1.4349e-01, 1.6184e-01, 8.0435e-01, 1.0000e+00],\n",
       "                       [5.0564e-02, 6.2430e-01, 5.4778e-01, 8.3953e-01],\n",
       "                       [2.0379e-01, 4.9458e-03, 9.5406e-01, 4.0314e-01],\n",
       "                       [0.0000e+00, 1.8817e-01, 8.9681e-01, 8.8713e-01],\n",
       "                       [0.0000e+00, 9.6432e-04, 5.6017e-01, 2.9680e-01],\n",
       "                       [0.0000e+00, 6.4291e-01, 8.9599e-01, 9.9285e-01],\n",
       "                       [0.0000e+00, 3.7776e-02, 8.5312e-01, 6.9220e-01],\n",
       "                       [0.0000e+00, 1.0485e-01, 6.7713e-01, 8.5282e-01],\n",
       "                       [0.0000e+00, 3.2029e-01, 8.3310e-01, 9.7313e-01],\n",
       "                       [1.8417e-01, 2.5708e-02, 9.5059e-01, 2.3801e-01],\n",
       "                       [0.0000e+00, 7.5267e-01, 7.2368e-01, 9.3065e-01],\n",
       "                       [3.0472e-01, 3.5080e-03, 1.0000e+00, 3.4167e-01],\n",
       "                       [0.0000e+00, 2.1232e-02, 6.1439e-01, 6.6333e-01],\n",
       "                       [7.5642e-02, 5.4435e-01, 9.9814e-01, 9.8944e-01],\n",
       "                       [1.5106e-01, 6.6965e-01, 3.7677e-01, 8.2308e-01],\n",
       "                       [4.4521e-01, 4.2018e-02, 1.0000e+00, 9.8559e-01],\n",
       "                       [1.5596e-01, 7.2936e-01, 9.8651e-01, 8.9237e-01]]]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, img_data in enumerate(train_data):\n",
    "    print(img_data)\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"On idx \" + str(idx))\n",
    "    image = np.array(img_data[\"img\"])\n",
    "    # run frcnn\n",
    "    images, sizes, scales_yx = image_preprocess(image)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=frcnn_cfg.max_detections,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    features = output_dict.get(\"roi_features\")\n",
    "    \n",
    "#     visual_embeddings.append(features)\n",
    "#     labels.append(img_data[\"label\"])\n",
    "    break\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fbfed74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['normalized_boxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1cefb1e-2d87-4137-b44a-86089935763b",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 embeddings\n",
      "On idx 0\n",
      "On idx 1000\n",
      "On idx 2000\n",
      "On idx 3000\n",
      "On idx 4000\n",
      "On idx 5000\n",
      "On idx 6000\n",
      "On idx 7000\n",
      "On idx 8000\n",
      "On idx 9000\n",
      "On idx 10000\n",
      "On idx 11000\n",
      "On idx 12000\n",
      "On idx 13000\n",
      "On idx 14000\n",
      "On idx 15000\n",
      "On idx 16000\n",
      "On idx 17000\n",
      "On idx 18000\n",
      "On idx 19000\n",
      "On idx 20000\n",
      "On idx 21000\n",
      "On idx 22000\n",
      "On idx 23000\n",
      "On idx 24000\n",
      "On idx 25000\n",
      "On idx 26000\n",
      "On idx 27000\n",
      "On idx 28000\n",
      "On idx 29000\n",
      "On idx 30000\n",
      "On idx 31000\n",
      "On idx 32000\n",
      "On idx 33000\n",
      "On idx 34000\n",
      "On idx 35000\n",
      "On idx 36000\n",
      "On idx 37000\n",
      "On idx 38000\n",
      "On idx 39000\n",
      "On idx 40000\n",
      "On idx 41000\n",
      "On idx 42000\n",
      "On idx 43000\n",
      "On idx 44000\n",
      "On idx 45000\n",
      "On idx 46000\n",
      "On idx 47000\n",
      "On idx 48000\n",
      "On idx 49000\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating \" + str(len(train_data)) + \" embeddings\")\n",
    "\n",
    "visual_embeddings = []\n",
    "labels = []\n",
    "bounding_boxes = []\n",
    "\n",
    "for idx, img_data in enumerate(train_data):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"On idx \" + str(idx))\n",
    "    image = np.array(img_data[\"img\"])\n",
    "    # run frcnn\n",
    "    images, sizes, scales_yx = image_preprocess(image)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=frcnn_cfg.max_detections,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    features = output_dict.get(\"roi_features\")\n",
    "    box = output_dict.get(\"normalized_boxes\")\n",
    "    \n",
    "    visual_embeddings.append(features)\n",
    "    bounding_boxes.append(box)\n",
    "    labels.append(img_data[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e6e6b20-0223-4776-ba26-2ca42da7a9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar10_train_embeddings = {\n",
    "    \"embeddings\": visual_embeddings,\n",
    "    \"bounding_box\": bounding_boxes,\n",
    "    \"labels\": labels\n",
    "}\n",
    "\n",
    "with open(\"cifar10-train-embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cifar10_train_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64eed64b-6a16-460c-bf6b-a84f1ab7eb64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 embeddings\n",
      "On idx 0\n",
      "On idx 1000\n",
      "On idx 2000\n",
      "On idx 3000\n",
      "On idx 4000\n",
      "On idx 5000\n",
      "On idx 6000\n",
      "On idx 7000\n",
      "On idx 8000\n",
      "On idx 9000\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating \" + str(len(test_data)) + \" embeddings\")\n",
    "\n",
    "visual_embeddings = []\n",
    "labels = []\n",
    "bounding_boxes = []\n",
    "\n",
    "for idx, img_data in enumerate(test_data):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"On idx \" + str(idx))\n",
    "    image = np.array(img_data[\"img\"])\n",
    "    # run frcnn\n",
    "    images, sizes, scales_yx = image_preprocess(image)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=frcnn_cfg.max_detections,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    features = output_dict.get(\"roi_features\")\n",
    "    box = output_dict.get(\"normalized_boxes\")\n",
    "    \n",
    "    visual_embeddings.append(features)\n",
    "    bounding_boxes.append(box)\n",
    "    labels.append(img_data[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77d322f0-0da5-46a6-9cec-f1c52cbdaf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar10_test_embeddings = {\n",
    "    \"embeddings\": visual_embeddings,\n",
    "    \"bounding_box\": bounding_boxes,\n",
    "    \"labels\": labels\n",
    "}\n",
    "\n",
    "with open(\"cifar10-test-embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cifar10_test_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7cb6dd-b6ec-43b7-a312-10cd13a0bbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
