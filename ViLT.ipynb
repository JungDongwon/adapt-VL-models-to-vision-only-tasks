{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltForQuestionAnswering, ViltConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, image_files, text, processor, num_labels):\n",
    "        self.image_files = image_files\n",
    "        self.text = text\n",
    "        self.processor = processor\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text\n",
    "        image = self.image_files[idx]['image']\n",
    "        label = self.image_files[idx]['label']\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        targets = torch.zeros(self.num_labels)\n",
    "        targets[label] = 1\n",
    "        encoding[\"labels\"] = targets\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/dongwon/dl-project/./cache/Maysee___parquet/Maysee--tiny-imagenet-35af7c46a941f08e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01612687110900879,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb57a37c44e4dcb88ab1fffa5ab05e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir='./cache'\n",
    "datasets = load_dataset('Maysee/tiny-imagenet', cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/dongwon/dl-project/./cache/Maysee___parquet/Maysee--tiny-imagenet-35af7c46a941f08e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012384176254272461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca2a67030c7477f8e68aec6cc52a76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 03:24:56.674358: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-27 03:24:57.433970: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:/opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:\n",
      "2023-03-27 03:24:57.434075: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:/opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:\n",
      "2023-03-27 03:24:57.434082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViltProcessor\n",
    "\n",
    "datasets = load_dataset('Maysee/tiny-imagenet', cache_dir=cache_dir)\n",
    "label_list = datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "config = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\", cache_dir=cache_dir)\n",
    "config.id2label = {str(i): label for i, label in enumerate(label_list)}\n",
    "config.label2id = {label: i for i, label in enumerate(label_list)}\n",
    "config.num_labels = num_labels\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "train_dataset = ImageDataset(image_files=datasets[\"train\"], text=\"\", processor=processor, num_labels=num_labels)\n",
    "test_dataset = ImageDataset(image_files=datasets[\"valid\"], text=\"\", processor=processor, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n01443537'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.nonzero(train_dataset[0]['labels']).squeeze().tolist()\n",
    "config.id2label[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dandelin/vilt-b32-mlm were not used when initializing ViltForQuestionAnswering: ['mlm_score.transform.LayerNorm.bias', 'mlm_score.transform.dense.bias', 'mlm_score.bias', 'mlm_score.transform.dense.weight', 'mlm_score.decoder.weight', 'mlm_score.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing ViltForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViltForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.3.weight', 'classifier.0.bias', 'classifier.0.weight', 'classifier.3.bias', 'classifier.1.bias', 'classifier.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViltForQuestionAnswering(\n",
       "  (vilt): ViltModel(\n",
       "    (embeddings): ViltEmbeddings(\n",
       "      (text_embeddings): TextEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768)\n",
       "        (position_embeddings): Embedding(40, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (patch_embeddings): ViltPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViltEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViltPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate=none)\n",
       "    (3): Linear(in_features=1536, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\", config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "  input_ids = [item['input_ids'] for item in batch]\n",
    "  pixel_values = [item['pixel_values'] for item in batch]\n",
    "  attention_mask = [item['attention_mask'] for item in batch]\n",
    "  token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "  labels = [item['labels'] for item in batch]\n",
    "\n",
    "  # create padded pixel values and corresponding pixel mask\n",
    "  encoding = processor.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "\n",
    "  # create new batch\n",
    "  batch = {}\n",
    "  batch['input_ids'] = torch.stack(input_ids)\n",
    "  batch['attention_mask'] = torch.stack(attention_mask)\n",
    "  batch['token_type_ids'] = torch.stack(token_type_ids)\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = torch.stack(labels)\n",
    "\n",
    "  return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=512, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, device, test_dataloader):\n",
    "    losses = []  # List of scalar tensors\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        # adapt batch to model\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        target = torch.argmax(batch['labels'], dim=1)\n",
    "        correct += torch.sum(preds==target).item()\n",
    "        total += target.size(0)\n",
    "        losses.append(outputs.loss)\n",
    "    stacked_losses = torch.stack(losses)  # (num_batches, ) \n",
    "    total_avg_loss = stacked_losses.mean()  # (num test examples, ) -> scalar\n",
    "    total_avg_acc = (100 * correct / total) / len(test_dataloader)\n",
    "\n",
    "    print(\"Correct: \" + str(correct), \"Total: \" + str(total))\n",
    "    print(\"Average test loss: \" + str(total_avg_loss.item()))\n",
    "    print(\"Average val acc: \" + str(total_avg_acc))\n",
    "\n",
    "    return total_avg_loss.item(), total_avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012438297271728516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 196,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee2d2e51d824051adbec7e04921ab3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongwon/.local/lib/python3.9/site-packages/transformers/models/vilt/processing_vilt.py:142: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "/home/dongwon/.local/lib/python3.9/site-packages/transformers/models/vilt/image_processing_vilt.py:357: FutureWarning: This method is deprecated and will be removed in v4.26.0. Please use pad instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 147.2014923095703\n",
      "Loss: 139.98004150390625\n",
      "Loss: 133.02212524414062\n",
      "Loss: 126.48741149902344\n",
      "Loss: 120.21019744873047\n",
      "Loss: 114.32279205322266\n",
      "Loss: 108.85628509521484\n",
      "Loss: 103.52584075927734\n",
      "Loss: 98.59742736816406\n",
      "Loss: 93.79312133789062\n",
      "Loss: 89.43826293945312\n",
      "Loss: 84.97572326660156\n",
      "Loss: 80.98971557617188\n",
      "Loss: 77.19768524169922\n",
      "Loss: 73.51335906982422\n",
      "Loss: 69.97216033935547\n",
      "Loss: 66.87934112548828\n",
      "Loss: 63.72492980957031\n",
      "Loss: 60.83019256591797\n",
      "Loss: 58.08111572265625\n",
      "Loss: 55.35917282104492\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012639522552490234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d736ea7eca1a4d859e213825826f193a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 59 Total: 10000\n",
      "Average test loss: 52.93903732299805\n",
      "Average val acc: 0.0295\n",
      "Loss: 53.00183868408203\n",
      "Loss: 50.65126419067383\n",
      "Loss: 48.451698303222656\n",
      "Loss: 46.34100341796875\n",
      "Loss: 44.51832580566406\n",
      "Loss: 42.55536651611328\n",
      "Loss: 40.86403274536133\n",
      "Loss: 39.21149826049805\n",
      "Loss: 37.70921325683594\n",
      "Loss: 36.152984619140625\n",
      "Loss: 34.865142822265625\n",
      "Loss: 33.618507385253906\n",
      "Loss: 32.32726287841797\n",
      "Loss: 31.28366470336914\n",
      "Loss: 30.14713478088379\n",
      "Loss: 29.18904685974121\n",
      "Loss: 28.186481475830078\n",
      "Loss: 27.207284927368164\n",
      "Loss: 26.405488967895508\n",
      "Loss: 25.61708641052246\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01262211799621582,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abca6d910b645969c595fb671563d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 76 Total: 10000\n",
      "Average test loss: 24.795358657836914\n",
      "Average val acc: 0.038\n",
      "Loss: 24.788639068603516\n",
      "Loss: 24.038055419921875\n",
      "Loss: 23.361705780029297\n",
      "Loss: 22.713186264038086\n",
      "Loss: 22.100696563720703\n",
      "Loss: 21.47056770324707\n",
      "Loss: 21.00933074951172\n",
      "Loss: 20.425806045532227\n",
      "Loss: 19.95071029663086\n",
      "Loss: 19.37655258178711\n",
      "Loss: 18.98367691040039\n",
      "Loss: 18.53857421875\n",
      "Loss: 18.109874725341797\n",
      "Loss: 17.779251098632812\n",
      "Loss: 17.323556900024414\n",
      "Loss: 16.935157775878906\n",
      "Loss: 16.63144874572754\n",
      "Loss: 16.31053352355957\n",
      "Loss: 15.967021942138672\n",
      "Loss: 15.695990562438965\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012885808944702148,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a0ded16f1e4a8980c7368915a0536b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 56 Total: 10000\n",
      "Average test loss: 15.385418891906738\n",
      "Average val acc: 0.028000000000000004\n",
      "Loss: 15.422800064086914\n",
      "Loss: 15.146252632141113\n",
      "Loss: 14.873952865600586\n",
      "Loss: 14.59813117980957\n",
      "Loss: 14.312643051147461\n",
      "Loss: 14.126028060913086\n",
      "Loss: 13.892168998718262\n",
      "Loss: 13.663530349731445\n",
      "Loss: 13.448955535888672\n",
      "Loss: 13.231642723083496\n",
      "Loss: 13.101935386657715\n",
      "Loss: 12.88783073425293\n",
      "Loss: 12.722521781921387\n",
      "Loss: 12.516234397888184\n",
      "Loss: 12.371763229370117\n",
      "Loss: 12.191215515136719\n",
      "Loss: 12.065913200378418\n",
      "Loss: 11.898890495300293\n",
      "Loss: 11.78935432434082\n",
      "Loss: 11.622722625732422\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012711048126220703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f7343acb54c19a682b4100f00ede3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 51 Total: 10000\n",
      "Average test loss: 11.476115226745605\n",
      "Average val acc: 0.025500000000000002\n",
      "Loss: 11.49130916595459\n",
      "Loss: 11.375856399536133\n",
      "Loss: 11.230683326721191\n",
      "Loss: 11.127729415893555\n",
      "Loss: 11.028316497802734\n",
      "Loss: 10.860284805297852\n",
      "Loss: 10.759739875793457\n",
      "Loss: 10.693625450134277\n",
      "Loss: 10.605430603027344\n",
      "Loss: 10.440827369689941\n",
      "Loss: 10.361474990844727\n",
      "Loss: 10.255295753479004\n",
      "Loss: 10.174308776855469\n",
      "Loss: 10.100034713745117\n",
      "Loss: 9.993803024291992\n",
      "Loss: 9.904325485229492\n",
      "Loss: 9.841365814208984\n",
      "Loss: 9.758225440979004\n",
      "Loss: 9.662202835083008\n",
      "Loss: 9.680790901184082\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012639045715332031,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d1eabf0fe54f79a355ebf580cf0eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 44 Total: 10000\n",
      "Average test loss: 9.526552200317383\n",
      "Average val acc: 0.022\n",
      "Loss: 9.521499633789062\n",
      "Loss: 9.466958045959473\n",
      "Loss: 9.413127899169922\n",
      "Loss: 9.317559242248535\n",
      "Loss: 9.253666877746582\n",
      "Loss: 9.178423881530762\n",
      "Loss: 9.114645004272461\n",
      "Loss: 9.057684898376465\n",
      "Loss: 9.027060508728027\n",
      "Loss: 8.955873489379883\n",
      "Loss: 8.936420440673828\n",
      "Loss: 8.818607330322266\n",
      "Loss: 8.810651779174805\n",
      "Loss: 8.73857593536377\n",
      "Loss: 8.696014404296875\n",
      "Loss: 8.637091636657715\n",
      "Loss: 8.612071990966797\n",
      "Loss: 8.563379287719727\n",
      "Loss: 8.526222229003906\n",
      "Loss: 8.460796356201172\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012288093566894531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b16c709fb3e4e06aded862b213a1c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 51 Total: 10000\n",
      "Average test loss: 8.422137260437012\n",
      "Average val acc: 0.025500000000000002\n",
      "Loss: 8.43097972869873\n",
      "Loss: 8.400518417358398\n",
      "Loss: 8.315138816833496\n",
      "Loss: 8.313461303710938\n",
      "Loss: 8.262186050415039\n",
      "Loss: 8.221821784973145\n",
      "Loss: 8.196314811706543\n",
      "Loss: 8.177116394042969\n",
      "Loss: 8.080995559692383\n",
      "Loss: 8.094027519226074\n",
      "Loss: 8.047892570495605\n",
      "Loss: 8.009960174560547\n",
      "Loss: 7.98164701461792\n",
      "Loss: 7.962636470794678\n",
      "Loss: 7.938411712646484\n",
      "Loss: 7.872116565704346\n",
      "Loss: 7.864894866943359\n",
      "Loss: 7.823638439178467\n",
      "Loss: 7.811345100402832\n",
      "Loss: 7.7826409339904785\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013663530349731445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead6987bf4e6436482bf3dbdf366db37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 59 Total: 10000\n",
      "Average test loss: 7.744096279144287\n",
      "Average val acc: 0.0295\n",
      "Loss: 7.754395961761475\n",
      "Loss: 7.720660209655762\n",
      "Loss: 7.685206890106201\n",
      "Loss: 7.6867499351501465\n",
      "Loss: 7.642859935760498\n",
      "Loss: 7.605317115783691\n",
      "Loss: 7.605081558227539\n",
      "Loss: 7.564018726348877\n",
      "Loss: 7.52272891998291\n",
      "Loss: 7.532293319702148\n",
      "Loss: 7.496265888214111\n",
      "Loss: 7.47697639465332\n",
      "Loss: 7.467223167419434\n",
      "Loss: 7.415633678436279\n",
      "Loss: 7.4150800704956055\n",
      "Loss: 7.414430618286133\n",
      "Loss: 7.390464782714844\n",
      "Loss: 7.355417251586914\n",
      "Loss: 7.3402099609375\n",
      "Loss: 7.329250335693359\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012362241744995117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5336895129241418be1a861aa202d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 57 Total: 10000\n",
      "Average test loss: 7.302845001220703\n",
      "Average val acc: 0.028499999999999998\n",
      "Loss: 7.319746017456055\n",
      "Loss: 7.266488552093506\n",
      "Loss: 7.253535747528076\n",
      "Loss: 7.247821807861328\n",
      "Loss: 7.216864109039307\n",
      "Loss: 7.228365421295166\n",
      "Loss: 7.235970497131348\n",
      "Loss: 7.181740760803223\n",
      "Loss: 7.174409866333008\n",
      "Loss: 7.179937362670898\n",
      "Loss: 7.147481918334961\n",
      "Loss: 7.116377353668213\n",
      "Loss: 7.102487087249756\n",
      "Loss: 7.09318208694458\n",
      "Loss: 7.069350719451904\n",
      "Loss: 7.076513767242432\n",
      "Loss: 7.044258117675781\n",
      "Loss: 7.054025650024414\n",
      "Loss: 7.074855804443359\n",
      "Loss: 7.025371551513672\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012392759323120117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b4b454078249d49c680f4563df3c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 63 Total: 10000\n",
      "Average test loss: 7.005491733551025\n",
      "Average val acc: 0.0315\n",
      "Loss: 7.029352188110352\n",
      "Loss: 6.995093822479248\n",
      "Loss: 6.969025135040283\n",
      "Loss: 6.968323707580566\n",
      "Loss: 6.967830657958984\n",
      "Loss: 6.9497294425964355\n",
      "Loss: 6.921489715576172\n",
      "Loss: 6.920053005218506\n",
      "Loss: 6.914517402648926\n",
      "Loss: 6.902112007141113\n",
      "Loss: 6.896595001220703\n",
      "Loss: 6.854294300079346\n",
      "Loss: 6.876458168029785\n",
      "Loss: 6.846006870269775\n",
      "Loss: 6.8603386878967285\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name or 'pooler' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "checkpoint_dir = './checkpoints'\n",
    "best_test_acc = -1\n",
    "step = -1\n",
    "model.train()\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "   print(f\"Epoch: {epoch}\")\n",
    "   for batch in tqdm(train_dataloader):\n",
    "        step += 1\n",
    "        # get the inputs; \n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        print(\"Loss:\", loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate\n",
    "        if step != 0 and step % 20 == 0:\n",
    "            model.eval()\n",
    "            new_test_loss, new_test_acc = evaluate(model, device, test_dataloader)\n",
    "\n",
    "            # save checkpoint with best test loss\n",
    "            if new_test_acc > best_test_acc or best_test_acc < 0:\n",
    "                if best_test_acc > 0:\n",
    "                    os.remove(checkpoint_dir + '/'+ best_checkpoint_filename)\n",
    "                best_checkpoint_filename = \"best_model\" + str(step) +\".pt\"\n",
    "                torch.save(model.state_dict(), checkpoint_dir + '/' + best_checkpoint_filename)\n",
    "                best_test_acc = new_test_acc\n",
    "\n",
    "            model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
