{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf65592-c8d0-4381-aaa0-45c39b5e1888",
   "metadata": {},
   "source": [
    "Here we get only the embedding for the top box (selected with scores as in modeling_frcnn.py) and reshape it to the expected shape\n",
    "\n",
    "The idea is that maybe such an embedding is more useful for an image classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebcf31be-2c72-4f3b-8e8e-4d4df20eec01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import PIL.Image\n",
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "from processing_image import Preprocess\n",
    "from visualizing_image import SingleImageViz\n",
    "from modeling_frcnn import GeneralizedRCNN\n",
    "from utils import Config\n",
    "import utils\n",
    "from transformers import VisualBertForQuestionAnswering, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f639b7-5fa5-4e11-9dde-e594c519a460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Image\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a96d43-0d18-4750-893b-7b51ded57ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "561d4619-ad18-467f-8412-a020e7c69011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22185457-c800-4aaa-9283-77c6414e1bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%s not found in cache or force_download set to True, downloading to %s https://s3.amazonaws.com/models.huggingface.co/bert/unc-nlp/frcnn-vg-finetuned/config.yaml /root/.cache/torch/transformers/tmp1rs7q487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41188bd3c5bb45ceba5c0718f2395b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "%s not found in cache or force_download set to True, downloading to %s https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin /root/.cache/torch/transformers/tmp74s2y058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4107284110e14b72bad5358ddcb6e90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/262M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /root/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load models and model components\n",
    "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "frcnn_cfg.model.DEVICE = device\n",
    "\n",
    "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg).to(device)\n",
    "\n",
    "image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "# bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "# visualbert_vqa = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5973abc-974e-4b1a-959f-64fc7c191ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734d05a31e5a4c7593ef9131c6bd342b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d8f1f3ce724894ad97ed05f8135d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb6e7ee56bd41359073686e87a97e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cifar100/cifar100 to /root/.cache/huggingface/datasets/cifar100/cifar100/1.0.0/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcb3c89c7b04e228a6b40b60ebb8c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/169M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/features/image.py:325: UserWarning: Downcasting array dtype uint8 to uint8 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cifar100 downloaded and prepared to /root/.cache/huggingface/datasets/cifar100/cifar100/1.0.0/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067dc6609864473787f243bcd01b83fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar100_data = load_dataset(\"cifar100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd5b6644-b809-403c-9ce8-9077529028fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = cifar100_data[\"train\"]\n",
    "test_data = cifar100_data[\"test\"]\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91047daa-74a0-4c42-867c-7df321df311f",
   "metadata": {},
   "source": [
    "First get the target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12554202-689d-44f6-99e1-c73dd28cd9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a single embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating a single embedding\")\n",
    "\n",
    "one_embedding = None\n",
    "\n",
    "for idx, img_data in enumerate(train_data):\n",
    "    image = np.array(img_data[\"img\"])\n",
    "    # run frcnn\n",
    "    images, sizes, scales_yx = image_preprocess(image)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=frcnn_cfg.max_detections,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    features = output_dict.get(\"roi_features\")\n",
    "    one_embedding = features\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9856a66-f8d7-43db-b31a-c96cb6988549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 2048])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "target_shape = one_embedding.shape\n",
    "print(target_shape)\n",
    "print(type(one_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1cefb1e-2d87-4137-b44a-86089935763b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 embeddings\n",
      "On idx 0\n",
      "On idx 1000\n",
      "On idx 2000\n",
      "On idx 3000\n",
      "On idx 4000\n",
      "On idx 5000\n",
      "On idx 6000\n",
      "On idx 7000\n",
      "On idx 8000\n",
      "On idx 9000\n",
      "On idx 10000\n",
      "On idx 11000\n",
      "On idx 12000\n",
      "On idx 13000\n",
      "On idx 14000\n",
      "On idx 15000\n",
      "On idx 16000\n",
      "On idx 17000\n",
      "On idx 18000\n",
      "On idx 19000\n",
      "On idx 20000\n",
      "On idx 21000\n",
      "On idx 22000\n",
      "On idx 23000\n",
      "On idx 24000\n",
      "On idx 25000\n",
      "On idx 26000\n",
      "On idx 27000\n",
      "On idx 28000\n",
      "On idx 29000\n",
      "On idx 30000\n",
      "On idx 31000\n",
      "On idx 32000\n",
      "On idx 33000\n",
      "On idx 34000\n",
      "On idx 35000\n",
      "On idx 36000\n",
      "On idx 37000\n",
      "On idx 38000\n",
      "On idx 39000\n",
      "On idx 40000\n",
      "On idx 41000\n",
      "On idx 42000\n",
      "On idx 43000\n",
      "On idx 44000\n",
      "On idx 45000\n",
      "On idx 46000\n",
      "On idx 47000\n",
      "On idx 48000\n",
      "On idx 49000\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating \" + str(len(train_data)) + \" embeddings\")\n",
    "\n",
    "visual_embeddings = []\n",
    "fine_labels = []\n",
    "coarse_labels = []\n",
    "\n",
    "for idx, img_data in enumerate(train_data):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"On idx \" + str(idx))\n",
    "    image = np.array(img_data[\"img\"])\n",
    "    # run frcnn\n",
    "    images, sizes, scales_yx = image_preprocess(image)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=1,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    features = output_dict.get(\"roi_features\")\n",
    "    \n",
    "    visual_embeddings.append(features.repeat(1,36,1))\n",
    "    fine_labels.append(img_data[\"fine_label\"])\n",
    "    coarse_labels.append(img_data[\"coarse_label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e6e6b20-0223-4776-ba26-2ca42da7a9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar100_train_embeddings = {\n",
    "    \"embeddings\": visual_embeddings,\n",
    "    \"fine_labels\": fine_labels,\n",
    "    \"coarse_labels\": coarse_labels\n",
    "}\n",
    "\n",
    "with open(\"cifar100-train-embeddings-topbox.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cifar100_train_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64eed64b-6a16-460c-bf6b-a84f1ab7eb64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 embeddings\n",
      "On idx 0\n",
      "On idx 1000\n",
      "On idx 2000\n",
      "On idx 3000\n",
      "On idx 4000\n",
      "On idx 5000\n",
      "On idx 6000\n",
      "On idx 7000\n",
      "On idx 8000\n",
      "On idx 9000\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating \" + str(len(test_data)) + \" embeddings\")\n",
    "\n",
    "visual_embeddings = []\n",
    "fine_labels = []\n",
    "coarse_labels = []\n",
    "\n",
    "for idx, img_data in enumerate(test_data):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"On idx \" + str(idx))\n",
    "    image = np.array(img_data[\"img\"])\n",
    "    # run frcnn\n",
    "    images, sizes, scales_yx = image_preprocess(image)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=1,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    features = output_dict.get(\"roi_features\")\n",
    "    \n",
    "    visual_embeddings.append(features.repeat(1,36,1))\n",
    "    fine_labels.append(img_data[\"fine_label\"])\n",
    "    coarse_labels.append(img_data[\"coarse_label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77d322f0-0da5-46a6-9cec-f1c52cbdaf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar100_test_embeddings = {\n",
    "    \"embeddings\": visual_embeddings,\n",
    "    \"fine_labels\": fine_labels,\n",
    "    \"coarse_labels\": coarse_labels\n",
    "}\n",
    "\n",
    "with open(\"cifar100-test-embeddings-topbox.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cifar100_test_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7cb6dd-b6ec-43b7-a312-10cd13a0bbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
