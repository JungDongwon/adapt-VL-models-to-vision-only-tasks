{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372955fe-ec44-4c9f-8a51-e8ce886a0c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import VisualBertModel, VisualBertConfig, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5356f74-5823-4e97-a9aa-0237bcef8b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cifar10Dataset(Dataset):\n",
    "    def __init__(self, visual_embeddings, labels, tokenizer):\n",
    "        self.visual_embeddings = visual_embeddings\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_labels = len(set(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        visual_embedding = self.visual_embeddings[idx]\n",
    "        # text_inputs = tokenizer(text_inputs, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        text_inputs = tokenizer(\"The image belongs to one of the following classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\", padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # remove batch dimension\n",
    "        for k,v in text_inputs.items():\n",
    "            text_inputs[k] = v.squeeze()\n",
    "            \n",
    "        label = torch.zeros(self.num_labels)\n",
    "        label[self.labels[idx]] = 1\n",
    "        return visual_embedding, text_inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a1be139-70bd-4405-b131-3cad1f86c08f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "visual_bert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
    "print(visual_bert.config.visual_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f640ec44-9ccb-4884-9248-c6f1809e706d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cd6ae5-26f0-415a-8be7-127ff22aa314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisualBERTForImageClassification(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.visual_bert = VisualBertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=self.visual_bert.config.hidden_size, out_features=1536, bias=True),\n",
    "            nn.LayerNorm((1536,), eps=1e-05, elementwise_affine=True),\n",
    "            nn.GELU(approximate='none'),\n",
    "            nn.Linear(in_features=1536, out_features=num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, visual_embeddings, text_inputs):         \n",
    "        outputs = self.visual_bert(\n",
    "            input_ids=text_inputs[\"input_ids\"],\n",
    "            attention_mask=text_inputs[\"attention_mask\"],\n",
    "            token_type_ids=text_inputs[\"token_type_ids\"],\n",
    "            visual_embeds=visual_embeddings,\n",
    "            visual_attention_mask=torch.ones(visual_embeddings.shape[:-1]).to(device),\n",
    "        )\n",
    "        return self.classifier(outputs.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5e15767-69ac-4830-aa2c-e4c3dd826a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "        visual_embeddings, text_inputs, labels = zip(*batch)\n",
    "        \n",
    "        # Flatten the visual embeddings\n",
    "        # visual_embeddings = [embedding.view(embedding.size(0), -1) for embedding in visual_embeddings]\n",
    "        visual_embeddings = [embedding.view(embedding.size(1), -1) for embedding in visual_embeddings]\n",
    "        \n",
    "        visual_embeddings = torch.stack(visual_embeddings)\n",
    "        \n",
    "        labels = torch.stack(labels)\n",
    "        \n",
    "        input_ids = torch.stack([item['input_ids'] for item in text_inputs])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in text_inputs])\n",
    "        token_type_ids = torch.stack([item['token_type_ids'] for item in text_inputs])\n",
    "        \n",
    "        text_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "        \n",
    "                \n",
    "        return visual_embeddings, text_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb860d71-c8f0-4d2b-bdfd-98c3e56ecbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for visual_embeddings, text_inputs, labels in dataloader:\n",
    "        visual_embeddings = visual_embeddings.to(device)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(visual_embeddings, text_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e5b18e-cac3-4371-bad6-9e8ffd7fd98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('cifar10-train-embeddings.pkl', 'rb') as f:\n",
    "    train_embeddings_all = pickle.load(f)\n",
    "\n",
    "with open('cifar10-test-embeddings.pkl', 'rb') as f:\n",
    "    test_embeddings_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546a6b05-14f4-46d4-a35a-6dbbaad2b750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_embeddings = train_embeddings_all['embeddings']\n",
    "train_labels = train_embeddings_all['labels']\n",
    "\n",
    "val_embeddings = test_embeddings_all['embeddings']\n",
    "val_labels = test_embeddings_all['labels']\n",
    "num_classes = len(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca050273-5818-400c-8985-0f588c187180",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 2048])\n",
      "torch.Size([1, 36, 2048])\n",
      "torch.Size([1, 36, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(train_embeddings[0].shape)\n",
    "print(train_embeddings[635].shape)\n",
    "print(train_embeddings[9121].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46b0c921-186b-484b-974a-260a9e2d138f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = VisualBERTForImageClassification(\"uclanlp/visualbert-vqa-coco-pre\", num_classes=num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd0b7c36-7df4-4c54-986b-c9099f98353b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisualBERTForImageClassification(\n",
       "  (visual_bert): VisualBertModel(\n",
       "    (embeddings): VisualBertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (visual_token_type_embeddings): Embedding(2, 768)\n",
       "      (visual_position_embeddings): Embedding(512, 768)\n",
       "      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
       "    )\n",
       "    (encoder): VisualBertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): VisualBertLayer(\n",
       "          (attention): VisualBertAttention(\n",
       "            (self): VisualBertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): VisualBertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VisualBertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VisualBertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): VisualBertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=1536, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2a1fbd-d43b-4f79-9866-e9d0c6b798f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Cifar10Dataset(train_embeddings, train_labels, tokenizer)\n",
    "val_dataset = Cifar10Dataset(val_embeddings, val_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33566460-d37b-4f63-ab84-a04e7055753a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name or 'pooler' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11323984-950b-4bb7-be0d-2e05c4b6c2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 196/196 [01:12<00:00,  2.70it/s]\n",
      "Epoch 2/30:  40%|████      | 79/196 [00:29<00:43,  2.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(visual_embeddings, text_inputs)\n\u001b[1;32m     26\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m train_predictions\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     29\u001b[0m train_labels_eval\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=6e-4)\n",
    "num_epochs = 30\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.25)\n",
    "\n",
    "best_params = None\n",
    "best_val_accuracy = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    train_predictions = []\n",
    "    train_labels_eval = []\n",
    "    \n",
    "    for visual_embeddings, text_inputs, labels in tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        visual_embeddings = visual_embeddings.to(device)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        outputs = model(visual_embeddings, text_inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        train_predictions.extend(preds.cpu().numpy())\n",
    "        train_labels_eval.extend(labels.cpu().numpy())\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels_eval = []\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for visual_embeddings, text_inputs, labels in val_dataloader:\n",
    "            visual_embeddings = visual_embeddings.to(device)\n",
    "            text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(visual_embeddings, text_inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            val_predictions.extend(preds.cpu().numpy())\n",
    "            val_labels_eval.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_labels_idx = [np.argmax(tensor) for tensor in val_labels_eval]\n",
    "    val_accuracy = accuracy_score(val_labels_idx, val_predictions)\n",
    "    \n",
    "    train_labels_idx = [np.argmax(tensor) for tensor in train_labels_eval]\n",
    "    train_accuracy = accuracy_score(train_labels_idx, train_predictions)\n",
    "    \n",
    "    # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Training Acc: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    with open(\"Question-AllClassesListed-C10-Results.txt\", 'a') as f:\n",
    "        to_write = f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Training Acc: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\"\n",
    "        f.write(to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa929e-68b2-4643-b957-6d3002fe00f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
