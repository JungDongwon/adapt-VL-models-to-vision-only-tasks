{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372955fe-ec44-4c9f-8a51-e8ce886a0c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Image\n",
    "\n",
    "from transformers import BlipForQuestionAnswering, BlipProcessor, BlipConfig, BlipModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7920a9b0-75ee-423a-9b7b-ccdda33fb84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5356f74-5823-4e97-a9aa-0237bcef8b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, image_files, text, processor, num_labels):\n",
    "        self.image_files = image_files\n",
    "        self.text = text\n",
    "        self.processor = processor\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text\n",
    "        image = self.image_files[idx]['img']\n",
    "        label = self.image_files[idx]['fine_label']\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        # encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        encoding = self.processor(image, text, return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        targets = torch.zeros(self.num_labels)\n",
    "        targets[label] = 1\n",
    "        encoding[\"labels\"] = targets\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938f9fe6-fb4d-497e-89f9-868c971850c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar100 (/root/.cache/huggingface/datasets/cifar100/cifar100/1.0.0/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed83eca448942799b1bf1df1cde8b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('cifar100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59785229-aebb-414a-99e5-ecf75d4a3ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list = dataset[\"train\"].features[\"fine_label\"].names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "config.id2label = {str(i): label for i, label in enumerate(label_list)}\n",
    "config.label2id = {label: str(i) for i, label in enumerate(label_list)}\n",
    "config.num_labels = num_labels\n",
    "config.max_length = 1\n",
    "config.text_config.max_length = 1\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "train_dataset = ImageDataset(image_files=dataset[\"train\"], text=\"The image belongs to one of the following classes: beaver, dolphin, otter, seal, whale ,aquarium fish, flatfish, ray, shark, trout, orchids, poppies, roses, sunflowers, tulips, bottles, bowls, cans, cups, plates, apples, mushrooms, oranges, pears, sweet peppers, clock, computer keyboard, lamp, telephone, television, bed, chair, couch, table, wardrobe, bee, beetle, butterfly, caterpillar, cockroach, bear, leopard, lion, tiger, wolf, bridge, castle, house, road, skyscraper, cloud, forest, mountain, plain, sea, camel, cattle, chimpanzee, elephant, kangaroo, fox, porcupine, possum, raccoon, skunk, crab, lobster, snail, spider, worm, baby, boy, girl, man, woman, crocodile, dinosaur, lizard, snake, turtle, hamster, mouse, rabbit, shrew, squirrel, maple, oak, palm, pine, willow, bicycle, bus, motorcycle, pickup truck, train, lawn-mower, rocket, streetcar, tank, tractor.\", processor=processor, num_labels=num_labels)\n",
    "test_dataset = ImageDataset(image_files=dataset[\"test\"], text=\"The image belongs to one of the following classes: beaver, dolphin, otter, seal, whale ,aquarium fish, flatfish, ray, shark, trout, orchids, poppies, roses, sunflowers, tulips, bottles, bowls, cans, cups, plates, apples, mushrooms, oranges, pears, sweet peppers, clock, computer keyboard, lamp, telephone, television, bed, chair, couch, table, wardrobe, bee, beetle, butterfly, caterpillar, cockroach, bear, leopard, lion, tiger, wolf, bridge, castle, house, road, skyscraper, cloud, forest, mountain, plain, sea, camel, cattle, chimpanzee, elephant, kangaroo, fox, porcupine, possum, raccoon, skunk, crab, lobster, snail, spider, worm, baby, boy, girl, man, woman, crocodile, dinosaur, lizard, snake, turtle, hamster, mouse, rabbit, shrew, squirrel, maple, oak, palm, pine, willow, bicycle, bus, motorcycle, pickup truck, train, lawn-mower, rocket, streetcar, tank, tractor.\", processor=processor, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d075fe15-aab5-4f41-a245-98dc6015cc4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", config=config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce999c9-d7b9-4b7a-bbc1-d4b569a2be7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  input_ids = [item['input_ids'] for item in batch]\n",
    "  pixel_values = [item['pixel_values'] for item in batch]\n",
    "  attention_mask = [item['attention_mask'] for item in batch]\n",
    "  # token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "  labels = [item['labels'] for item in batch]\n",
    "\n",
    "  # create padded pixel values and corresponding pixel mask\n",
    "  # encoding = processor.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "\n",
    "  # create new batch\n",
    "  batch = {}\n",
    "  batch['input_ids'] = torch.stack(input_ids)\n",
    "  batch['attention_mask'] = torch.stack(attention_mask)\n",
    "  # batch['token_type_ids'] = torch.stack(token_type_ids)\n",
    "  # batch['pixel_values'] = encoding['pixel_values']\n",
    "  # batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['pixel_values'] = torch.stack(pixel_values)\n",
    "  batch['labels'] = torch.stack(labels)\n",
    "  # batch['labels'] = torch.Tensor(labels).type(torch.LongTensor).unsqueeze(1)\n",
    "\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70cfc7e-517f-4e5f-944d-547af369a38f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33566460-d37b-4f63-ab84-a04e7055753a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3f2a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2604d24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cls_model = nn.Sequential(\n",
    "    nn.Linear(in_features=model.text_decoder.config.vocab_size, out_features=num_labels, bias=True)\n",
    ")\n",
    "cls_model = cls_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4446838-c178-4f36-ab96-f4111a1360e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (1) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   0%|          | 1/391 [00:05<33:25,  5.14s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   1%|          | 2/391 [00:08<27:01,  4.17s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   1%|          | 3/391 [00:12<24:56,  3.86s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   1%|          | 4/391 [00:15<24:02,  3.73s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   1%|▏         | 5/391 [00:20<25:38,  3.99s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   2%|▏         | 6/391 [00:24<26:44,  4.17s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   2%|▏         | 7/391 [00:28<25:40,  4.01s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   2%|▏         | 8/391 [00:32<25:52,  4.05s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   2%|▏         | 9/391 [00:38<28:53,  4.54s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   3%|▎         | 10/391 [00:45<34:10,  5.38s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   3%|▎         | 11/391 [00:53<38:36,  6.10s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   3%|▎         | 12/391 [00:58<37:52,  5.99s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   3%|▎         | 13/391 [01:02<33:24,  5.30s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   4%|▎         | 14/391 [01:06<31:05,  4.95s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   4%|▍         | 15/391 [01:11<31:18,  5.00s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   4%|▍         | 16/391 [01:18<34:37,  5.54s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   4%|▍         | 17/391 [01:26<39:08,  6.28s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   5%|▍         | 18/391 [01:32<39:12,  6.31s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   5%|▍         | 19/391 [01:37<35:11,  5.68s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   5%|▌         | 20/391 [01:41<31:50,  5.15s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   5%|▌         | 21/391 [01:46<32:14,  5.23s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   6%|▌         | 22/391 [01:53<36:03,  5.86s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   6%|▌         | 23/391 [02:01<39:43,  6.48s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   6%|▌         | 24/391 [02:07<38:19,  6.27s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   6%|▋         | 25/391 [02:10<33:02,  5.42s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   7%|▋         | 26/391 [02:15<30:57,  5.09s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   7%|▋         | 27/391 [02:21<32:32,  5.36s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   7%|▋         | 28/391 [02:28<36:04,  5.96s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   7%|▋         | 29/391 [02:36<39:13,  6.50s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   8%|▊         | 30/391 [02:41<36:41,  6.10s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   8%|▊         | 31/391 [02:45<32:11,  5.37s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   8%|▊         | 32/391 [02:49<30:07,  5.04s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   8%|▊         | 33/391 [02:55<32:08,  5.39s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   9%|▊         | 34/391 [03:03<36:28,  6.13s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   9%|▉         | 35/391 [03:09<36:56,  6.23s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   9%|▉         | 36/391 [03:13<32:41,  5.53s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:   9%|▉         | 37/391 [03:17<30:03,  5.09s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  10%|▉         | 38/391 [03:23<30:40,  5.21s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  10%|▉         | 39/391 [03:30<34:13,  5.83s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  10%|█         | 40/391 [03:38<37:58,  6.49s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  10%|█         | 41/391 [03:44<36:20,  6.23s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  11%|█         | 42/391 [03:47<31:35,  5.43s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  11%|█         | 43/391 [03:52<29:22,  5.07s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  11%|█▏        | 44/391 [03:58<31:27,  5.44s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  12%|█▏        | 45/391 [04:06<35:13,  6.11s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  12%|█▏        | 46/391 [04:13<37:08,  6.46s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  12%|█▏        | 47/391 [04:18<34:33,  6.03s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  12%|█▏        | 48/391 [04:22<30:19,  5.31s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  13%|█▎        | 49/391 [04:26<28:41,  5.04s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  13%|█▎        | 50/391 [04:32<30:55,  5.44s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  13%|█▎        | 51/391 [04:40<34:42,  6.12s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  13%|█▎        | 52/391 [04:47<35:49,  6.34s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  14%|█▎        | 53/391 [04:52<33:13,  5.90s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  14%|█▍        | 54/391 [04:55<29:24,  5.24s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  14%|█▍        | 55/391 [05:01<29:34,  5.28s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  14%|█▍        | 56/391 [05:08<32:11,  5.76s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  15%|█▍        | 57/391 [05:16<36:19,  6.52s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  15%|█▍        | 58/391 [05:22<35:52,  6.46s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  15%|█▌        | 59/391 [05:26<31:19,  5.66s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  15%|█▌        | 60/391 [05:30<28:48,  5.22s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  16%|█▌        | 61/391 [05:36<29:09,  5.30s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  16%|█▌        | 62/391 [05:43<32:04,  5.85s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  16%|█▌        | 63/391 [05:51<36:01,  6.59s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  16%|█▋        | 64/391 [05:57<34:47,  6.38s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  17%|█▋        | 65/391 [06:01<29:51,  5.50s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  17%|█▋        | 66/391 [06:05<27:53,  5.15s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  17%|█▋        | 67/391 [06:11<29:30,  5.47s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  17%|█▋        | 68/391 [06:19<32:39,  6.07s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  18%|█▊        | 69/391 [06:26<35:19,  6.58s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  18%|█▊        | 70/391 [06:32<33:35,  6.28s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  18%|█▊        | 71/391 [06:36<29:06,  5.46s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  18%|█▊        | 72/391 [06:40<27:04,  5.09s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  19%|█▊        | 73/391 [06:45<27:54,  5.27s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  19%|█▉        | 74/391 [06:53<31:03,  5.88s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  19%|█▉        | 75/391 [07:01<34:19,  6.52s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  19%|█▉        | 76/391 [07:07<33:15,  6.34s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  20%|█▉        | 77/391 [07:11<29:28,  5.63s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  20%|█▉        | 78/391 [07:15<26:58,  5.17s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  20%|██        | 79/391 [07:20<27:05,  5.21s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  20%|██        | 80/391 [07:27<29:38,  5.72s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  21%|██        | 81/391 [07:35<33:12,  6.43s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  21%|██        | 82/391 [07:42<33:40,  6.54s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  21%|██        | 83/391 [07:46<30:37,  5.96s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  21%|██▏       | 84/391 [07:50<27:03,  5.29s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  22%|██▏       | 85/391 [07:55<25:44,  5.05s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  22%|██▏       | 86/391 [08:01<26:57,  5.30s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  22%|██▏       | 87/391 [08:08<29:53,  5.90s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  23%|██▎       | 88/391 [08:15<32:20,  6.40s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  23%|██▎       | 89/391 [08:21<30:34,  6.07s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  23%|██▎       | 90/391 [08:24<26:46,  5.34s/it]Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Epoch 1/30:  23%|██▎       | 91/391 [08:29<24:59,  5.00s/it]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cls_model.parameters(), lr=6e-4)\n",
    "num_epochs = 30\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.25)\n",
    "\n",
    "best_params = None\n",
    "best_val_accuracy = -1\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    train_predictions = []\n",
    "    train_labels_eval = []\n",
    "    step = 0\n",
    "    \n",
    "    for batch in tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        \n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        outputs = model.generate(**batch)  # N,2\n",
    "        outputs = outputs[:,1]\n",
    "        outputs = nn.functional.one_hot(outputs, num_classes = model.text_decoder.config.vocab_size).type(torch.FloatTensor)\n",
    "        outputs = outputs.to(device)\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs_cls = cls_model(outputs)\n",
    "        loss = criterion(outputs_cls, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs_cls, 1)\n",
    "        train_predictions.extend(preds.cpu().numpy())\n",
    "        train_labels_eval.extend(labels.cpu().numpy())\n",
    "        \n",
    "        with open(\"BLIP-AllClasses-C100.txt\", 'a') as f:\n",
    "            f.write(f\"Batch: {step} --- Loss: {loss}\\n\")\n",
    "\n",
    "        total_loss += loss\n",
    "        step += 1\n",
    "            \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels_eval = []\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        step = 0\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            \n",
    "            outputs = model.generate(**batch)\n",
    "            outputs = outputs[:,1]\n",
    "            outputs = nn.functional.one_hot(outputs, num_classes = model.text_decoder.config.vocab_size).type(torch.FloatTensor)\n",
    "            outputs = outputs.to(device)\n",
    "\n",
    "            outputs_cls = cls_model(outputs)\n",
    "            _, preds = torch.max(outputs_cls, 1)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            val_predictions.extend(preds.cpu().numpy())\n",
    "            val_labels_eval.extend(labels.cpu().numpy())\n",
    "\n",
    "            step += 1\n",
    "    \n",
    "    val_labels_idx = [np.argmax(tensor) for tensor in val_labels_eval]\n",
    "    val_accuracy = accuracy_score(val_labels_idx, val_predictions)\n",
    "    \n",
    "    train_labels_idx = [np.argmax(tensor) for tensor in train_labels_eval]\n",
    "    train_accuracy = accuracy_score(train_labels_idx, train_predictions)\n",
    "    \n",
    "    # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Training Acc: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    to_write = f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Training Acc: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\"\n",
    "    with open(\"BLIP-AllClasses-C100.txt\", 'a') as f:\n",
    "        f.write(to_write)\n",
    "    # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcca965-7b43-47c1-b857-e170a03b9ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
