{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372955fe-ec44-4c9f-8a51-e8ce886a0c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Image\n",
    "\n",
    "from transformers import BlipForQuestionAnswering, BlipProcessor, BlipConfig, BlipModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7920a9b0-75ee-423a-9b7b-ccdda33fb84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5356f74-5823-4e97-a9aa-0237bcef8b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, config, image_files, text, processor, num_labels):\n",
    "        self.config = config\n",
    "        self.image_files = image_files\n",
    "        self.text = text\n",
    "        self.processor = processor\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text\n",
    "        image = self.image_files[idx]['img']\n",
    "        label_id = self.image_files[idx]['fine_label']\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        # encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        encoding = self.processor(image, text, return_tensors=\"pt\")\n",
    "        label_encoding = self.processor(text=self.config.id2label[str(label_id)], padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "        label_encoding = label_encoding.squeeze()\n",
    "\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        #targets = torch.zeros(self.num_labels)\n",
    "        #targets[label_id] = 1\n",
    "        encoding[\"labels\"] = label_encoding\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938f9fe6-fb4d-497e-89f9-868c971850c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar100 (/home/dongwon/adapt-VL-models-to-vision-only-tasks/Nish/BLIP Experiments/./cache/cifar100/cifar100/1.0.0/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016443729400634766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b10e4a10de4093844523ee1f77e4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('cifar100', cache_dir='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59785229-aebb-414a-99e5-ecf75d4a3ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 09:41:15.711022: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 09:41:16.414232: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:/opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:\n",
      "2023-04-24 09:41:16.414343: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:/opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:\n",
      "2023-04-24 09:41:16.414349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple tensor([[ 101, 6207,  102]])\n",
      "aquarium_fish tensor([[  101, 18257,  1035,  3869,   102]])\n",
      "baby tensor([[ 101, 3336,  102]])\n",
      "bear tensor([[ 101, 4562,  102]])\n",
      "beaver tensor([[  101, 13570,   102]])\n",
      "bed tensor([[ 101, 2793,  102]])\n",
      "bee tensor([[  101, 10506,   102]])\n",
      "beetle tensor([[ 101, 7813,  102]])\n",
      "bicycle tensor([[  101, 10165,   102]])\n",
      "bottle tensor([[ 101, 5835,  102]])\n",
      "bowl tensor([[ 101, 4605,  102]])\n",
      "boy tensor([[ 101, 2879,  102]])\n",
      "bridge tensor([[ 101, 2958,  102]])\n",
      "bus tensor([[ 101, 3902,  102]])\n",
      "butterfly tensor([[ 101, 9112,  102]])\n",
      "camel tensor([[  101, 19130,   102]])\n",
      "can tensor([[ 101, 2064,  102]])\n",
      "castle tensor([[ 101, 3317,  102]])\n",
      "caterpillar tensor([[  101, 23488,  8197, 17305,   102]])\n",
      "cattle tensor([[ 101, 7125,  102]])\n",
      "chair tensor([[ 101, 3242,  102]])\n",
      "chimpanzee tensor([[  101,  9610,  8737,  2319, 23940,   102]])\n",
      "clock tensor([[ 101, 5119,  102]])\n",
      "cloud tensor([[ 101, 6112,  102]])\n",
      "cockroach tensor([[  101, 10338,  3217,  6776,   102]])\n",
      "couch tensor([[ 101, 6411,  102]])\n",
      "crocodile tensor([[  101, 21843,   102]])\n",
      "cup tensor([[ 101, 2452,  102]])\n",
      "dinosaur tensor([[  101, 15799,   102]])\n",
      "dolphin tensor([[  101, 17801,   102]])\n",
      "elephant tensor([[  101, 10777,   102]])\n",
      "flatfish tensor([[ 101, 4257, 7529,  102]])\n",
      "forest tensor([[ 101, 3224,  102]])\n",
      "fox tensor([[ 101, 4419,  102]])\n",
      "girl tensor([[ 101, 2611,  102]])\n",
      "hamster tensor([[  101, 10654,  6238,   102]])\n",
      "house tensor([[ 101, 2160,  102]])\n",
      "kangaroo tensor([[  101, 21652,   102]])\n",
      "keyboard tensor([[ 101, 9019,  102]])\n",
      "lamp tensor([[  101, 10437,   102]])\n",
      "lawn_mower tensor([[  101, 10168,  1035,  9587, 13777,   102]])\n",
      "leopard tensor([[  101, 16240,   102]])\n",
      "lion tensor([[ 101, 7006,  102]])\n",
      "lizard tensor([[  101, 15450,   102]])\n",
      "lobster tensor([[  101, 27940,   102]])\n",
      "man tensor([[ 101, 2158,  102]])\n",
      "maple_tree tensor([[  101, 11035,  1035,  3392,   102]])\n",
      "motorcycle tensor([[ 101, 9055,  102]])\n",
      "mountain tensor([[ 101, 3137,  102]])\n",
      "mouse tensor([[ 101, 8000,  102]])\n",
      "mushroom tensor([[  101, 18565,   102]])\n",
      "oak_tree tensor([[ 101, 6116, 1035, 3392,  102]])\n",
      "orange tensor([[ 101, 4589,  102]])\n",
      "orchid tensor([[  101, 15573,   102]])\n",
      "otter tensor([[  101, 22279,   102]])\n",
      "palm_tree tensor([[ 101, 5340, 1035, 3392,  102]])\n",
      "pear tensor([[  101, 28253,   102]])\n",
      "pickup_truck tensor([[  101, 15373,  1035,  4744,   102]])\n",
      "pine_tree tensor([[ 101, 7222, 1035, 3392,  102]])\n",
      "plain tensor([[ 101, 5810,  102]])\n",
      "plate tensor([[ 101, 5127,  102]])\n",
      "poppy tensor([[  101, 19745,   102]])\n",
      "porcupine tensor([[  101, 18499, 15569,  3170,   102]])\n",
      "possum tensor([[  101, 13433,  4757,  2819,   102]])\n",
      "rabbit tensor([[  101, 10442,   102]])\n",
      "raccoon tensor([[  101, 10958, 21408,  2239,   102]])\n",
      "ray tensor([[ 101, 4097,  102]])\n",
      "road tensor([[ 101, 2346,  102]])\n",
      "rocket tensor([[ 101, 7596,  102]])\n",
      "rose tensor([[ 101, 3123,  102]])\n",
      "sea tensor([[ 101, 2712,  102]])\n",
      "seal tensor([[ 101, 7744,  102]])\n",
      "shark tensor([[  101, 11420,   102]])\n",
      "shrew tensor([[  101, 14021, 15603,   102]])\n",
      "skunk tensor([[  101, 15315, 16814,   102]])\n",
      "skyscraper tensor([[  101, 24581,   102]])\n",
      "snail tensor([[  101, 10879,   102]])\n",
      "snake tensor([[ 101, 7488,  102]])\n",
      "spider tensor([[ 101, 6804,  102]])\n",
      "squirrel tensor([[  101, 18197,   102]])\n",
      "streetcar tensor([[  101, 21420,   102]])\n",
      "sunflower tensor([[  101,  3103, 14156,   102]])\n",
      "sweet_pepper tensor([[  101,  4086,  1035, 11565,   102]])\n",
      "table tensor([[ 101, 2795,  102]])\n",
      "tank tensor([[ 101, 4951,  102]])\n",
      "telephone tensor([[ 101, 7026,  102]])\n",
      "television tensor([[ 101, 2547,  102]])\n",
      "tiger tensor([[ 101, 6816,  102]])\n",
      "tractor tensor([[  101, 16358,   102]])\n",
      "train tensor([[ 101, 3345,  102]])\n",
      "trout tensor([[  101, 13452,   102]])\n",
      "tulip tensor([[  101, 10722, 15000,   102]])\n",
      "turtle tensor([[  101, 13170,   102]])\n",
      "wardrobe tensor([[  101, 17828,   102]])\n",
      "whale tensor([[  101, 13156,   102]])\n",
      "willow_tree tensor([[  101, 11940,  1035,  3392,   102]])\n",
      "wolf tensor([[ 101, 4702,  102]])\n",
      "woman tensor([[ 101, 2450,  102]])\n",
      "worm tensor([[  101, 15485,   102]])\n",
      "crab tensor([[  101, 18081,   102]])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].features[\"fine_label\"].names\n",
    "\n",
    "# replace label 'cra' to 'crab'\n",
    "label_list.remove('cra')\n",
    "label_list.append('crab')\n",
    "num_labels = len(label_list)\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "max_label_token_length = 0\n",
    "for label in label_list:\n",
    "    labels = processor(text=label, return_tensors=\"pt\").input_ids\n",
    "    max_label_token_length = max(max_label_token_length, len(labels[0]))\n",
    "    print(label, labels)\n",
    "print(max_label_token_length)\n",
    "\n",
    "config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "config.id2label = {str(i): label for i, label in enumerate(label_list)}\n",
    "config.label2id = {label: str(i) for i, label in enumerate(label_list)}\n",
    "config.num_labels = num_labels\n",
    "#config.max_length = max_label_token_length\n",
    "config.text_config.max_length = max_label_token_length # for CLS and SEP tokens\n",
    "\n",
    "\n",
    "train_dataset = ImageDataset(config, image_files=dataset[\"train\"], text=\"\", processor=processor, num_labels=num_labels)\n",
    "test_dataset = ImageDataset(config, image_files=dataset[\"test\"], text=\"\", processor=processor, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a05efa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = ['beaver', 'dolphin', 'otter', 'seal', 'whale', 'aquarium fish', 'flatfish', 'ray', 'shark', 'trout', 'orchids', 'poppies', 'roses', 'sunflowers', 'tulips', 'bottles', 'bowls', 'cans', 'cups', 'plates', 'apples', 'mushrooms', 'oranges', 'pears', 'sweet peppers', 'clock', 'computer keyboard', 'lamp', 'telephone', 'television', 'bed', 'chair', 'couch', 'table', 'wardrobe', 'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach', 'bear', 'leopard', 'lion', 'tiger', 'wolf', 'bridge', 'castle', 'house', 'road', 'skyscraper', 'cloud', 'forest', 'mountain', 'plain', 'sea', 'camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo', 'fox', 'porcupine', 'possum', 'raccoon', 'skunk', 'crab', 'lobster', 'snail', 'spider', 'worm', 'baby', 'boy', 'girl', 'man', 'woman', 'crocodile', 'dinosaur', 'lizard', 'snake', 'turtle', 'hamster', 'mouse', 'rabbit', 'shrew', 'squirrel', 'maple', 'oak', 'palm', 'pine', 'willow', 'bicycle', 'bus', 'motorcycle', 'pickup truck', 'train', 'lawn-mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "real_labels = set()\n",
    "for label in dataset[\"train\"].features[\"fine_label\"].names:\n",
    "    real_labels.add(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c45e32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lion', 'snake', 'palm_tree', 'apple', 'cockroach', 'couch', 'house', 'maple_tree', 'mushroom', 'orange', 'bear', 'pine_tree', 'bus', 'kangaroo', 'snail', 'spider', 'mouse', 'whale', 'table', 'shrew', 'baby', 'lawn_mower', 'pickup_truck', 'pear', 'willow_tree', 'bottle', 'motorcycle', 'possum', 'plate', 'skunk', 'lobster', 'ray', 'rocket', 'caterpillar', 'bee', 'man', 'bowl', 'lizard', 'cup', 'squirrel', 'girl', 'camel', 'lamp', 'porcupine', 'crab', 'orchid', 'otter', 'wardrobe', 'rose', 'poppy', 'chair', 'tiger', 'clock', 'keyboard', 'trout', 'seal', 'worm', 'plain', 'aquarium_fish', 'mountain', 'bicycle', 'television', 'dolphin', 'oak_tree', 'cloud', 'tractor', 'beaver', 'bridge', 'fox', 'train', 'sweet_pepper', 'tulip', 'leopard', 'road', 'telephone', 'rabbit', 'chimpanzee', 'flatfish', 'raccoon', 'boy', 'hamster', 'castle', 'dinosaur', 'bed', 'sunflower', 'shark', 'turtle', 'wolf', 'beetle', 'forest', 'can', 'elephant', 'woman', 'cattle', 'sea', 'tank', 'butterfly', 'streetcar', 'crocodile', 'skyscraper'}\n"
     ]
    }
   ],
   "source": [
    "print(real_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d075fe15-aab5-4f41-a245-98dc6015cc4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", config=config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce999c9-d7b9-4b7a-bbc1-d4b569a2be7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  input_ids = [item['input_ids'] for item in batch]\n",
    "  pixel_values = [item['pixel_values'] for item in batch]\n",
    "  attention_mask = [item['attention_mask'] for item in batch]\n",
    "  # token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "  labels = [item['labels'] for item in batch]\n",
    "\n",
    "  # create padded pixel values and corresponding pixel mask\n",
    "  # encoding = processor.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "\n",
    "  # create new batch\n",
    "  batch = {}\n",
    "  batch['input_ids'] = torch.stack(input_ids)\n",
    "  batch['attention_mask'] = torch.stack(attention_mask)\n",
    "  # batch['token_type_ids'] = torch.stack(token_type_ids)\n",
    "  # batch['pixel_values'] = encoding['pixel_values']\n",
    "  # batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['pixel_values'] = torch.stack(pixel_values)\n",
    "  batch['labels'] = torch.stack(labels)\n",
    "  # batch['labels'] = torch.Tensor(labels).type(torch.LongTensor).unsqueeze(1)\n",
    "\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70cfc7e-517f-4e5f-944d-547af369a38f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a41a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_decoder.cls.predictions.bias\n",
      "text_decoder.cls.predictions.transform.dense.weight\n",
      "text_decoder.cls.predictions.transform.dense.bias\n",
      "text_decoder.cls.predictions.transform.LayerNorm.weight\n",
      "text_decoder.cls.predictions.transform.LayerNorm.bias\n",
      "text_decoder.cls.predictions.decoder.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'text_decoder.cls' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33566460-d37b-4f63-ab84-a04e7055753a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_decoder.cls.predictions.bias\n",
      "text_decoder.cls.predictions.transform.dense.weight\n",
      "text_decoder.cls.predictions.transform.dense.bias\n",
      "text_decoder.cls.predictions.transform.LayerNorm.weight\n",
      "text_decoder.cls.predictions.transform.LayerNorm.bias\n",
      "text_decoder.cls.predictions.decoder.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'text_decoder.cls' in name:\n",
    "        print(name)\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e3f2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4446838-c178-4f36-ab96-f4111a1360e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:  22%|██▏       | 348/1563 [05:41<19:51,  1.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dongwon/adapt-VL-models-to-vision-only-tasks/Nish/BLIP Experiments/BLIP-EmptyString-Cifar10.ipynb Cell 14\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.10.195.51/home/dongwon/adapt-VL-models-to-vision-only-tasks/Nish/BLIP%20Experiments/BLIP-EmptyString-Cifar10.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.10.195.51/home/dongwon/adapt-VL-models-to-vision-only-tasks/Nish/BLIP%20Experiments/BLIP-EmptyString-Cifar10.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# import ipdb; ipdb.set_trace()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B52.10.195.51/home/dongwon/adapt-VL-models-to-vision-only-tasks/Nish/BLIP%20Experiments/BLIP-EmptyString-Cifar10.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.10.195.51/home/dongwon/adapt-VL-models-to-vision-only-tasks/Nish/BLIP%20Experiments/BLIP-EmptyString-Cifar10.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B52.10.195.51/home/dongwon/adapt-VL-models-to-vision-only-tasks/Nish/BLIP%20Experiments/BLIP-EmptyString-Cifar10.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/blip/modeling_blip.py:1212\u001b[0m, in \u001b[0;36mBlipForQuestionAnswering.forward\u001b[0;34m(self, input_ids, pixel_values, decoder_input_ids, decoder_attention_mask, attention_mask, output_attentions, output_hidden_states, labels, return_dict)\u001b[0m\n\u001b[1;32m   1209\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m image_attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(image_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m-> 1212\u001b[0m question_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_encoder(\n\u001b[1;32m   1213\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1214\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1215\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mimage_embeds,\n\u001b[1;32m   1216\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mimage_attention_mask,\n\u001b[1;32m   1217\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1218\u001b[0m )\n\u001b[1;32m   1220\u001b[0m question_embeds \u001b[39m=\u001b[39m question_embeds[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict \u001b[39melse\u001b[39;00m question_embeds\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m   1222\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     \u001b[39m# get decoder inputs from shifting lm labels to the right - this is used in training mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:774\u001b[0m, in \u001b[0;36mBlipTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     embedding_output \u001b[39m=\u001b[39m encoder_embeds\n\u001b[0;32m--> 774\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    775\u001b[0m     embedding_output,\n\u001b[1;32m    776\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    777\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    778\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    779\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    780\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    781\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    782\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    783\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    784\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    785\u001b[0m )\n\u001b[1;32m    786\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    787\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:433\u001b[0m, in \u001b[0;36mBlipTextEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    424\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    425\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    434\u001b[0m         hidden_states,\n\u001b[1;32m    435\u001b[0m         attention_mask,\n\u001b[1;32m    436\u001b[0m         layer_head_mask,\n\u001b[1;32m    437\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    438\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    439\u001b[0m         past_key_value,\n\u001b[1;32m    440\u001b[0m         output_attentions,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    443\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    444\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:350\u001b[0m, in \u001b[0;36mBlipTextLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    347\u001b[0m present_key_value \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     cross_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrossattention(\n\u001b[1;32m    351\u001b[0m         attention_output,\n\u001b[1;32m    352\u001b[0m         attention_mask,\n\u001b[1;32m    353\u001b[0m         head_mask,\n\u001b[1;32m    354\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    355\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    356\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m     attention_output \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    359\u001b[0m     outputs \u001b[39m=\u001b[39m outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39m# add cross attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:267\u001b[0m, in \u001b[0;36mBlipTextAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    258\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ):\n\u001b[0;32m--> 267\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    268\u001b[0m         hidden_states,\n\u001b[1;32m    269\u001b[0m         attention_mask,\n\u001b[1;32m    270\u001b[0m         head_mask,\n\u001b[1;32m    271\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    272\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    273\u001b[0m         past_key_value,\n\u001b[1;32m    274\u001b[0m         output_attentions,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    276\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    277\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/blip/modeling_blip_text.py:191\u001b[0m, in \u001b[0;36mBlipTextSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    188\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size)\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in BlipTextModel forward() function)\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\u001b[39m.\u001b[39;49mto(attention_scores\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    193\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)(attention_scores)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=6e-4)\n",
    "num_epochs = 30\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.25)\n",
    "\n",
    "best_params = None\n",
    "best_val_accuracy = -1\n",
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    step = 0\n",
    "    \n",
    "    for batch in tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        outputs = model(**batch) \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        total_loss += loss\n",
    "        step += 1\n",
    "            \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels_eval = []\n",
    "    total_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            \n",
    "            outputs = model.generate(**batch)\n",
    "            outputs = outputs[:,0:max_label_token_length]\n",
    "            #print('outputs:',processor.decode(outputs[0], skip_special_tokens=True), outputs[0])\n",
    "            labels = batch['labels'][:,0:max_label_token_length]\n",
    "            #print('labels:',processor.decode(labels[0], skip_special_tokens=True), labels[0])\n",
    "            outputs = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "            outputs = [output.replace(\" \", \"\") for output in outputs]\n",
    "            labels = [label.replace(\" \", \"\") for label in labels]\n",
    "            print('outputs:',outputs)\n",
    "            print('labels:',labels)\n",
    "\n",
    "            # count how many are correct\n",
    "            correct = 0\n",
    "            for i in range(len(outputs)):\n",
    "                if outputs[i] == labels[i]:\n",
    "                    correct += 1\n",
    "            acc = (correct * 100) / len(labels)\n",
    "            print(acc)\n",
    "\n",
    "            total_acc += acc\n",
    "        print('total acc:',total_acc / len(val_dataloader))\n",
    "            \n",
    "            #val_predictions.extend(preds.cpu().numpy())\n",
    "            #val_labels_eval.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "        #val_labels_idx = [np.argmax(tensor) for tensor in val_labels_eval]\n",
    "        #val_accuracy = accuracy_score(val_labels_idx, val_predictions)\n",
    "        \n",
    "        \n",
    "        #print(f\"Epoch {epoch + 1}/{num_epochs},  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be001c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
