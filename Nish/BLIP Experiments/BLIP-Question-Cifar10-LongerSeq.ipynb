{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372955fe-ec44-4c9f-8a51-e8ce886a0c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Image\n",
    "\n",
    "from transformers import BlipForQuestionAnswering, BlipProcessor, BlipConfig, BlipModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7920a9b0-75ee-423a-9b7b-ccdda33fb84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5356f74-5823-4e97-a9aa-0237bcef8b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, image_files, text, processor, num_labels):\n",
    "        self.image_files = image_files\n",
    "        self.text = text\n",
    "        self.processor = processor\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text\n",
    "        image = self.image_files[idx]['img']\n",
    "        label = self.image_files[idx]['label']\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        # encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        encoding = self.processor(image, text, return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        targets = torch.zeros(self.num_labels)\n",
    "        targets[label] = 1\n",
    "        encoding[\"labels\"] = targets\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938f9fe6-fb4d-497e-89f9-868c971850c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9fe6a4b4d04e29b27253313b7e6ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59785229-aebb-414a-99e5-ecf75d4a3ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list = dataset[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "config.id2label = {str(i): label for i, label in enumerate(label_list)}\n",
    "config.label2id = {label: str(i) for i, label in enumerate(label_list)}\n",
    "config.num_labels = num_labels\n",
    "config.max_length = 20\n",
    "config.text_config.max_length = 20\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "train_dataset = ImageDataset(image_files=dataset[\"train\"], text=\"What is this image?\", processor=processor, num_labels=num_labels)\n",
    "test_dataset = ImageDataset(image_files=dataset[\"test\"], text=\"What is this image?\", processor=processor, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d075fe15-aab5-4f41-a245-98dc6015cc4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", config=config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce999c9-d7b9-4b7a-bbc1-d4b569a2be7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  input_ids = [item['input_ids'] for item in batch]\n",
    "  pixel_values = [item['pixel_values'] for item in batch]\n",
    "  attention_mask = [item['attention_mask'] for item in batch]\n",
    "  # token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "  labels = [item['labels'] for item in batch]\n",
    "\n",
    "  # create padded pixel values and corresponding pixel mask\n",
    "  # encoding = processor.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "\n",
    "  # create new batch\n",
    "  batch = {}\n",
    "  batch['input_ids'] = torch.stack(input_ids)\n",
    "  batch['attention_mask'] = torch.stack(attention_mask)\n",
    "  # batch['token_type_ids'] = torch.stack(token_type_ids)\n",
    "  # batch['pixel_values'] = encoding['pixel_values']\n",
    "  # batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['pixel_values'] = torch.stack(pixel_values)\n",
    "  batch['labels'] = torch.stack(labels)\n",
    "  # batch['labels'] = torch.Tensor(labels).type(torch.LongTensor).unsqueeze(1)\n",
    "\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70cfc7e-517f-4e5f-944d-547af369a38f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33566460-d37b-4f63-ab84-a04e7055753a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3f2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2604d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a RNN to classify the output hidden state\n",
    "rnn_cls = nn.RNN(input_size=model.text_decoder.config.vocab_size, hidden_size=num_labels, num_layers = 1, batch_first=True)\n",
    "rnn_cls = rnn_cls.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4446838-c178-4f36-ab96-f4111a1360e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:  70%|██████▉   | 273/391 [13:15<05:38,  2.87s/it]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_cls.parameters(), lr=6e-4)\n",
    "num_epochs = 30\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.25)\n",
    "\n",
    "best_params = None\n",
    "best_val_accuracy = -1\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    train_predictions = []\n",
    "    train_labels_eval = []\n",
    "    step = 0\n",
    "    \n",
    "    for batch in tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        outputs = model.generate(**batch)  # N,20\n",
    "        outputs = outputs[:,1:]  # N,19\n",
    "        outputs = nn.functional.one_hot(outputs, num_classes = model.text_decoder.config.vocab_size).type(torch.FloatTensor)\n",
    "        outputs = outputs.to(device)  # N,19,vocab_size\n",
    "        labels = batch['labels']\n",
    "\n",
    "        _, outputs_cls = rnn_cls(outputs)  # (1, N, num_labels)\n",
    "        outputs_cls = torch.squeeze(outputs_cls, dim=0)  # (N, num_labels)\n",
    "        loss = criterion(outputs_cls, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs_cls, 1)\n",
    "        train_predictions.extend(preds.cpu().numpy())\n",
    "        train_labels_eval.extend(labels.cpu().numpy())\n",
    "\n",
    "        total_loss += loss\n",
    "        step += 1\n",
    "            \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels_eval = []\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        step = 0\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            \n",
    "            outputs = model.generate(**batch)\n",
    "            outputs = outputs[:,1]\n",
    "            outputs = nn.functional.one_hot(outputs, num_classes = model.text_decoder.config.vocab_size).type(torch.FloatTensor)\n",
    "            outputs = outputs.to(device)\n",
    "\n",
    "            _, outputs_cls = rnn_cls(outputs)  # (1, N, num_labels)\n",
    "            outputs_cls = torch.squeeze(outputs_cls, dim=0)  # (N, num_labels)\n",
    "            _, preds = torch.max(outputs_cls, 1)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            val_predictions.extend(preds.cpu().numpy())\n",
    "            val_labels_eval.extend(labels.cpu().numpy())\n",
    "\n",
    "            step += 1\n",
    "    \n",
    "    val_labels_idx = [np.argmax(tensor) for tensor in val_labels_eval]\n",
    "    val_accuracy = accuracy_score(val_labels_idx, val_predictions)\n",
    "    \n",
    "    train_labels_idx = [np.argmax(tensor) for tensor in train_labels_eval]\n",
    "    train_accuracy = accuracy_score(train_labels_idx, train_predictions)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Training Acc: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa5de6-89fc-4b81-a4f9-681094c57d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
