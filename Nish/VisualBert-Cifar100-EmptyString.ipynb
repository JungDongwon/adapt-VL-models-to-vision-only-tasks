{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "372955fe-ec44-4c9f-8a51-e8ce886a0c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import VisualBertModel, VisualBertConfig, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5356f74-5823-4e97-a9aa-0237bcef8b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cifar100Dataset(Dataset):\n",
    "    def __init__(self, visual_embeddings, labels, tokenizer):\n",
    "        self.visual_embeddings = visual_embeddings\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_labels = len(set(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        visual_embedding = self.visual_embeddings[idx]\n",
    "        # text_inputs = tokenizer(text_inputs, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        text_inputs = tokenizer(\"\", padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # remove batch dimension\n",
    "        for k,v in text_inputs.items():\n",
    "            text_inputs[k] = v.squeeze()\n",
    "            \n",
    "        label = torch.zeros(self.num_labels)\n",
    "        label[self.labels[idx]] = 1\n",
    "        return visual_embedding, text_inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1be139-70bd-4405-b131-3cad1f86c08f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-nlvr2\")\n",
    "visual_bert = VisualBertModel(config)\n",
    "visual_bert.config.visual_embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cd6ae5-26f0-415a-8be7-127ff22aa314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisualBERTForImageClassification(nn.Module):\n",
    "    def __init__(self, config, num_classes, visual_embedding_size):\n",
    "        super().__init__()\n",
    "        self.visual_bert = VisualBertModel(config)\n",
    "        # self.projection = nn.Linear(256*7*7, visual_embedding_size * 256)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, visual_embeddings, text_inputs):\n",
    "#         # Project to the size that VisualBert expects\n",
    "#         input_visual_embeddings = self.projection(visual_embeddings.view(visual_embeddings.size(0), -1))\n",
    "        \n",
    "#         # Reshape the visual embeddings back to 3D tensor\n",
    "#         input_visual_embeddings = input_visual_embeddings.view(input_visual_embeddings.size(0), 256, -1)\n",
    "                \n",
    "        outputs = self.visual_bert(\n",
    "            input_ids=text_inputs[\"input_ids\"],\n",
    "            attention_mask=text_inputs[\"attention_mask\"],\n",
    "            token_type_ids=text_inputs[\"token_type_ids\"],\n",
    "            visual_embeds=visual_embeddings\n",
    "        )\n",
    "        return self.classifier(outputs.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae008da-fdd9-4e1a-b974-8182bd02240f",
   "metadata": {
    "tags": []
   },
   "source": [
    "class CollateFunction:\n",
    "    def __init__(self, tokenizer, max_length=32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        visual_embeddings, text_inputs, labels = zip(*batch)\n",
    "        \n",
    "        # Flatten the visual embeddings\n",
    "        visual_embeddings = [embedding.view(256, -1) for embedding in visual_embeddings]\n",
    "        \n",
    "        visual_embeddings = torch.stack(visual_embeddings)\n",
    "        text_inputs = self.tokenizer(list(text_inputs), padding=\"max_length\", max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        return visual_embeddings, text_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e15767-69ac-4830-aa2c-e4c3dd826a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "        visual_embeddings, text_inputs, labels = zip(*batch)\n",
    "        \n",
    "        # Flatten the visual embeddings\n",
    "        visual_embeddings = [embedding.view(embedding.size(0), -1) for embedding in visual_embeddings]\n",
    "        \n",
    "        visual_embeddings = torch.stack(visual_embeddings)\n",
    "        \n",
    "        labels = torch.stack(labels)\n",
    "        \n",
    "        input_ids = torch.stack([item['input_ids'] for item in text_inputs])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in text_inputs])\n",
    "        token_type_ids = torch.stack([item['token_type_ids'] for item in text_inputs])\n",
    "        \n",
    "        text_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "        \n",
    "                \n",
    "        return visual_embeddings, text_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb860d71-c8f0-4d2b-bdfd-98c3e56ecbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for visual_embeddings, text_inputs, labels in dataloader:\n",
    "        visual_embeddings = visual_embeddings.to(device)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(visual_embeddings, text_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38505fb3-0cd8-4214-a1c8-eabb2485a470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('cifar100-train-embeddings.pkl', 'rb') as f:\n",
    "    train_embeddings_all = pickle.load(f)\n",
    "\n",
    "with open('cifar100-test-embeddings.pkl', 'rb') as f:\n",
    "    test_embeddings_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546a6b05-14f4-46d4-a35a-6dbbaad2b750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_embeddings = train_embeddings_all['embeddings']\n",
    "train_labels = train_embeddings_all['fine_labels']\n",
    "\n",
    "val_embeddings = test_embeddings_all['embeddings']\n",
    "val_labels = test_embeddings_all['fine_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d952428-93dd-4af8-978b-bbe7a31e6dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b251cf36-9ab2-493a-8263-da68d503dbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = len(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e90b71de-5ae1-4259-8558-1beb1c83f8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VisualBERTForImageClassification(config, num_classes=num_classes, visual_embedding_size=1024)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e2a1fbd-d43b-4f79-9866-e9d0c6b798f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Cifar100Dataset(train_embeddings, train_labels, tokenizer)\n",
    "val_dataset = Cifar100Dataset(val_embeddings, val_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e3ee648-96ff-4829-a2b6-d21a49992b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in model.visual_bert.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9526836e-2b4b-4585-9855-dba5bd4f0835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=5e-5)\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11323984-950b-4bb7-be0d-2e05c4b6c2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 347/347 [00:20<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 3.9592, Validation Accuracy: 0.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 347/347 [00:20<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 3.9043, Validation Accuracy: 0.2062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 347/347 [00:21<00:00, 16.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 3.8533, Validation Accuracy: 0.2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 347/347 [00:21<00:00, 16.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 3.8108, Validation Accuracy: 0.2095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 347/347 [00:22<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 3.7692, Validation Accuracy: 0.2151\n"
     ]
    }
   ],
   "source": [
    "best_params = None\n",
    "best_val_accuracy = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for visual_embeddings, text_inputs, labels in tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        visual_embeddings = visual_embeddings.to(device)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(visual_embeddings, text_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for visual_embeddings, text_inputs, labels in val_dataloader:\n",
    "            visual_embeddings = visual_embeddings.to(device)\n",
    "            text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(visual_embeddings, text_inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            val_predictions.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_labels_idx = [np.argmax(tensor) for tensor in val_labels]\n",
    "    val_accuracy = accuracy_score(val_labels_idx, val_predictions)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save a checkpoint if validation accuracy is greater than the previous best validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_params = model.state_dict()\n",
    "        torch.save(best_params, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a590da42-49f3-4042-9128-0f2f03e0bb55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([94, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings[1231].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b188e92e-ce41-4821-ae0a-7bc3bcb84286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lowest = 1000\n",
    "\n",
    "for e in train_embeddings:\n",
    "    lowest = min(lowest, e.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "274badb3-7f35-40f5-b4ad-e536aa0c861d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a71ea77-5e63-41bc-9629-b5d9d7f6e8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings[1231][:47].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe0de0b4-a717-4db6-a6de-322ade29f367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, e in enumerate(train_embeddings):\n",
    "    train_embeddings[idx] = e[:47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732c4cff-ff52-4624-b0d5-331d8b05cee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, e in enumerate(val_embeddings):\n",
    "    val_embeddings[idx] = e[:47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb6764-0482-41b3-8f07-873f6e9a286c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
