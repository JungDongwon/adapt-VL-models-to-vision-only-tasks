{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372955fe-ec44-4c9f-8a51-e8ce886a0c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import VisualBertModel, VisualBertConfig, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5356f74-5823-4e97-a9aa-0237bcef8b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cifar100Dataset(Dataset):\n",
    "    def __init__(self, visual_embeddings, labels, tokenizer):\n",
    "        self.visual_embeddings = visual_embeddings\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_labels = len(set(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        visual_embedding = self.visual_embeddings[idx]\n",
    "        # text_inputs = tokenizer(text_inputs, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        text_inputs = tokenizer(\"What is shown in this image?\", padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # remove batch dimension\n",
    "        for k,v in text_inputs.items():\n",
    "            text_inputs[k] = v.squeeze()\n",
    "            \n",
    "        label = torch.zeros(self.num_labels)\n",
    "        label[self.labels[idx]] = 1\n",
    "        return visual_embedding, text_inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1be139-70bd-4405-b131-3cad1f86c08f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-nlvr2\")\n",
    "visual_bert = VisualBertModel(config)\n",
    "visual_bert.config.visual_embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cd6ae5-26f0-415a-8be7-127ff22aa314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisualBERTForImageClassification(nn.Module):\n",
    "    def __init__(self, config, num_classes, visual_embedding_size):\n",
    "        super().__init__()\n",
    "        self.visual_bert = VisualBertModel(config)\n",
    "        # self.projection = nn.Linear(256*7*7, visual_embedding_size * 256)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, visual_embeddings, text_inputs):\n",
    "#         # Project to the size that VisualBert expects\n",
    "#         input_visual_embeddings = self.projection(visual_embeddings.view(visual_embeddings.size(0), -1))\n",
    "        \n",
    "#         # Reshape the visual embeddings back to 3D tensor\n",
    "#         input_visual_embeddings = input_visual_embeddings.view(input_visual_embeddings.size(0), 256, -1)\n",
    "                \n",
    "        outputs = self.visual_bert(\n",
    "            input_ids=text_inputs[\"input_ids\"],\n",
    "            attention_mask=text_inputs[\"attention_mask\"],\n",
    "            token_type_ids=text_inputs[\"token_type_ids\"],\n",
    "            visual_embeds=visual_embeddings\n",
    "        )\n",
    "        return self.classifier(outputs.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae008da-fdd9-4e1a-b974-8182bd02240f",
   "metadata": {
    "tags": []
   },
   "source": [
    "class CollateFunction:\n",
    "    def __init__(self, tokenizer, max_length=32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        visual_embeddings, text_inputs, labels = zip(*batch)\n",
    "        \n",
    "        # Flatten the visual embeddings\n",
    "        visual_embeddings = [embedding.view(256, -1) for embedding in visual_embeddings]\n",
    "        \n",
    "        visual_embeddings = torch.stack(visual_embeddings)\n",
    "        text_inputs = self.tokenizer(list(text_inputs), padding=\"max_length\", max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        return visual_embeddings, text_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e15767-69ac-4830-aa2c-e4c3dd826a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "        visual_embeddings, text_inputs, labels = zip(*batch)\n",
    "        \n",
    "        # Flatten the visual embeddings\n",
    "        visual_embeddings = [embedding.view(embedding.size(0), -1) for embedding in visual_embeddings]\n",
    "        \n",
    "        visual_embeddings = torch.stack(visual_embeddings)\n",
    "        \n",
    "        labels = torch.stack(labels)\n",
    "        \n",
    "        input_ids = torch.stack([item['input_ids'] for item in text_inputs])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in text_inputs])\n",
    "        token_type_ids = torch.stack([item['token_type_ids'] for item in text_inputs])\n",
    "        \n",
    "        text_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "        \n",
    "                \n",
    "        return visual_embeddings, text_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb860d71-c8f0-4d2b-bdfd-98c3e56ecbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for visual_embeddings, text_inputs, labels in dataloader:\n",
    "        visual_embeddings = visual_embeddings.to(device)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(visual_embeddings, text_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38505fb3-0cd8-4214-a1c8-eabb2485a470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('cifar100-train-embeddings.pkl', 'rb') as f:\n",
    "    train_embeddings_all = pickle.load(f)\n",
    "\n",
    "with open('cifar100-test-embeddings.pkl', 'rb') as f:\n",
    "    test_embeddings_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b4e3df5-e919-4964-addf-b3e9fe26656c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def curtail_embeddings(embeddings):\n",
    "    lowest = 1000\n",
    "    \n",
    "    for e in embeddings['embeddings']:\n",
    "        lowest = min(lowest, e.size(0))\n",
    "    \n",
    "    for idx, e in enumerate(embeddings['embeddings']):\n",
    "        embeddings['embeddings'][idx] = e[:lowest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ea68e2-0725-4bca-bb5c-0baed8445955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curtail_embeddings(train_embeddings_all)\n",
    "curtail_embeddings(test_embeddings_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546a6b05-14f4-46d4-a35a-6dbbaad2b750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_embeddings = train_embeddings_all['embeddings']\n",
    "train_labels = train_embeddings_all['fine_labels']\n",
    "\n",
    "val_embeddings = test_embeddings_all['embeddings']\n",
    "val_labels = test_embeddings_all['fine_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d952428-93dd-4af8-978b-bbe7a31e6dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b251cf36-9ab2-493a-8263-da68d503dbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = len(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e90b71de-5ae1-4259-8558-1beb1c83f8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VisualBERTForImageClassification(config, num_classes=num_classes, visual_embedding_size=1024)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2a1fbd-d43b-4f79-9866-e9d0c6b798f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Cifar100Dataset(train_embeddings, train_labels, tokenizer)\n",
    "val_dataset = Cifar100Dataset(val_embeddings, val_labels, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e3ee648-96ff-4829-a2b6-d21a49992b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in model.visual_bert.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9526836e-2b4b-4585-9855-dba5bd4f0835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=5e-5)\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11323984-950b-4bb7-be0d-2e05c4b6c2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 782/782 [01:00<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 4.5130, Validation Accuracy: 0.1263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 782/782 [01:00<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 4.3179, Validation Accuracy: 0.1577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 782/782 [01:00<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 4.1611, Validation Accuracy: 0.1790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 782/782 [01:00<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 4.0335, Validation Accuracy: 0.1988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 782/782 [01:00<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 3.9261, Validation Accuracy: 0.2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 782/782 [01:00<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 3.8373, Validation Accuracy: 0.2128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 782/782 [01:00<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 3.7611, Validation Accuracy: 0.2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 782/782 [01:00<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 3.6957, Validation Accuracy: 0.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 782/782 [01:00<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 3.6409, Validation Accuracy: 0.2321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 782/782 [01:00<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 3.5909, Validation Accuracy: 0.2361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 782/782 [01:00<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 3.5467, Validation Accuracy: 0.2415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 782/782 [01:00<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 3.5111, Validation Accuracy: 0.2454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 782/782 [01:00<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 3.4769, Validation Accuracy: 0.2449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 782/782 [01:00<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 3.4453, Validation Accuracy: 0.2533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 782/782 [01:00<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 3.4179, Validation Accuracy: 0.2528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 782/782 [01:00<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 3.3922, Validation Accuracy: 0.2535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 782/782 [01:00<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 3.3717, Validation Accuracy: 0.2589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 782/782 [01:00<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 3.3512, Validation Accuracy: 0.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 782/782 [01:00<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 3.3247, Validation Accuracy: 0.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 782/782 [01:00<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 3.3110, Validation Accuracy: 0.2624\n"
     ]
    }
   ],
   "source": [
    "best_params = None\n",
    "best_val_accuracy = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for visual_embeddings, text_inputs, labels in tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        visual_embeddings = visual_embeddings.to(device)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(visual_embeddings, text_inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for visual_embeddings, text_inputs, labels in val_dataloader:\n",
    "            visual_embeddings = visual_embeddings.to(device)\n",
    "            text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(visual_embeddings, text_inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            val_predictions.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_labels_idx = [np.argmax(tensor) for tensor in val_labels]\n",
    "    val_accuracy = accuracy_score(val_labels_idx, val_predictions)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save a checkpoint if validation accuracy is greater than the previous best validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_params = model.state_dict()\n",
    "        torch.save(best_params, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb6764-0482-41b3-8f07-873f6e9a286c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
